{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def safe_json_load(value):\n",
    "    if not value or pd.isna(value) or (isinstance(value, str) and value.strip() == \"\"):\n",
    "        return {}\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            return json.loads(value)\n",
    "        return value\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def analyze_json_references(data):\n",
    "    \"\"\"\n",
    "    Process a JSON object (or dict) to count and analyze properties\n",
    "    with 'referenced' and 'text_reference'. Returns a summary dictionary.\n",
    "    \"\"\"\n",
    "    true_count = 0\n",
    "    false_count = 0\n",
    "    false_details = {}  # Initialize as dict instead of list\n",
    "  \n",
    "    def process_json(obj, parent_key=None):\n",
    "        nonlocal true_count, false_count, false_details\n",
    "        if isinstance(obj, dict):\n",
    "            # Check if current dict has both \"referenced\" and \"text_reference\"\n",
    "            if \"referenced\" in obj and \"text_reference\" in obj:\n",
    "                if obj[\"referenced\"] is True:\n",
    "                    true_count += 1\n",
    "                else:\n",
    "                    false_count += 1\n",
    "                    # Use parent key if available; otherwise, 'unknown'\n",
    "                    prop_name = parent_key if parent_key is not None else 'unknown'\n",
    "                    false_details[prop_name] = obj[\"text_reference\"]\n",
    "            # Recursively process each key-value pair\n",
    "            for key, value in obj.items():\n",
    "                process_json(value, parent_key=key)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                process_json(item, parent_key=parent_key)\n",
    "\n",
    "    process_json(data)\n",
    "    total_references = true_count + false_count\n",
    "    percentage_true = (true_count / total_references * 100) if total_references else 0\n",
    "   \n",
    "    return {\n",
    "        \"prompt_related_score\": percentage_true,\n",
    "        \"total_references\": total_references,\n",
    "        \"true_references\": true_count,\n",
    "        \"false_references\": false_count,\n",
    "        \"false_references_details\": false_details\n",
    "    }\n",
    "\n",
    "def analyze_json_column(json_value):\n",
    "    \"\"\"\n",
    "    Wrapper function to process a DataFrame cell.\n",
    "    Expects json_value to be either a JSON string or a dict.\n",
    "    \"\"\"\n",
    "    if isinstance(json_value, str):\n",
    "        data = json.loads(json_value)\n",
    "    else:\n",
    "        data = json_value\n",
    "    return analyze_json_references(data)\n",
    "\n",
    "\n",
    "def extract_all_schema_keys(schema):\n",
    "    keys = set()\n",
    "    def traverse(obj, parent=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            # Traverse keys under \"properties\"\n",
    "            if \"properties\" in obj:\n",
    "                for key, value in obj[\"properties\"].items():\n",
    "                    # You can either capture the full path or just the key\n",
    "                    # Here, we capture just the key name\n",
    "                    keys.add(key)\n",
    "                    traverse(value, key)\n",
    "            # Also traverse \"additionalProperties\" if present\n",
    "            if \"additionalProperties\" in obj and isinstance(obj[\"additionalProperties\"], dict):\n",
    "                traverse(obj[\"additionalProperties\"], parent)\n",
    "            # Traverse oneOf, anyOf, allOf arrays if present\n",
    "            for combiner in [\"oneOf\", \"anyOf\", \"allOf\"]:\n",
    "                if combiner in obj and isinstance(obj[combiner], list):\n",
    "                    for item in obj[combiner]:\n",
    "                        traverse(item, parent)\n",
    "            # Traverse \"items\" (for arrays)\n",
    "            if \"items\" in obj:\n",
    "                traverse(obj[\"items\"], parent)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                traverse(item, parent)\n",
    "    traverse(schema)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def extract_referenced_json_keys_nonmetadata(data):\n",
    "    \"\"\"\n",
    "    Recursively extracts all keys from a JSON object as hierarchical dot paths,\n",
    "    but ignores common metadata keys like \"type\", \"referenced\", \"text_reference\",\n",
    "    and additional ones such as \"minimum\", \"maximum\", \"pattern\", \"enum\", \"format\", and \"items\".\n",
    "    \"\"\"\n",
    "    metadata_keys = {\"type\", \"referenced\", \"text_reference\", \"minimum\", \"maximum\", \"pattern\", \"enum\", \"format\", \"items\"}\n",
    "    keys = set()\n",
    "    \n",
    "    def traverse(obj, parent=\"\"):\n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in obj.items():\n",
    "                # Skip metadata keys\n",
    "                if key in metadata_keys:\n",
    "                    continue\n",
    "                current_path = f\"{parent}.{key}\" if parent else key\n",
    "                keys.add(current_path)\n",
    "                traverse(value, current_path)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                traverse(item, parent)\n",
    "    \n",
    "    traverse(data)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def analyze_json_and_schema(referenced_json, schema_json):\n",
    "    \"\"\"\n",
    "    Combines analysis of the referenced JSON with schema matching.\n",
    "    Returns the analysis dict with the following additional keys:\n",
    "      - schema_keys_count: number of keys (without parent paths) in the schema.\n",
    "      - referenced_json_key_count: number of keys (without parent paths) in the referenced JSON.\n",
    "      - matching_properties_count: number of keys that appear in both.\n",
    "      - non_matching_properties_count: number of keys in the schema that are not present in the referenced JSON.\n",
    "      - non_matching_properties: sorted list of keys (from the schema) that are missing in the referenced JSON.\n",
    "    \n",
    "    The original analysis from analyze_json_column(referenced_json) is preserved.\n",
    "    \"\"\"\n",
    "    # Get the original analysis from the referenced JSON.\n",
    "    analysis = analyze_json_column(referenced_json)\n",
    "    \n",
    "    # Use your helper functions that extract keys without parent paths.\n",
    "    schema_keys_str = get_sorted_schema_keys_without_parents(schema_json)\n",
    "    referenced_keys_str = get_referenced_json_keys_without_parents(referenced_json)\n",
    "    \n",
    "    # Convert the comma-separated strings into sets of keys.\n",
    "    schema_keys_set = set([k.strip() for k in schema_keys_str.split(\",\") if k.strip()])\n",
    "    referenced_keys_set = set([k.strip() for k in referenced_keys_str.split(\",\") if k.strip()])\n",
    "    \n",
    "    # Calculate key counts and matching statistics.\n",
    "    schema_keys_count = len(schema_keys_set)\n",
    "    referenced_json_key_count = len(referenced_keys_set)\n",
    "    matching_properties_count = len(schema_keys_set.intersection(referenced_keys_set))\n",
    "    \n",
    "    # Here we list only the keys defined in the schema that are missing in the referenced JSON.\n",
    "    non_matching = schema_keys_set - referenced_keys_set\n",
    "    non_matching_properties_count = len(non_matching)\n",
    "    \n",
    "    # Append the matching stats to the analysis.\n",
    "    analysis[\"schema_keys_count\"] = schema_keys_count\n",
    "    analysis[\"referenced_json_key_count\"] = referenced_json_key_count\n",
    "    analysis[\"matching_properties_count\"] = matching_properties_count\n",
    "    analysis[\"non_matching_properties_count\"] = non_matching_properties_count\n",
    "    analysis[\"non_matching_properties\"] = sorted(list(non_matching))\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Helper function to get sorted schema keys as a comma-separated string\n",
    "def get_sorted_schema_keys(schema_value):\n",
    "    schema_data = safe_json_load(schema_value)\n",
    "    keys = extract_all_schema_keys(schema_data)\n",
    "    return \", \".join(sorted(keys))\n",
    "\n",
    "# Helper function to get sorted data keys as a comma-separated string\n",
    "def get_referenced_json_keys(data_value):\n",
    "    data = safe_json_load(data_value)\n",
    "    keys = extract_referenced_json_keys_nonmetadata(data)\n",
    "    return \", \".join(sorted(keys))\n",
    "\n",
    "def get_sorted_schema_keys_without_parents(schema_value):\n",
    "    # Call the original function to get the full hierarchical keys as a comma-separated string\n",
    "    full_keys_str = get_sorted_schema_keys(schema_value)\n",
    "    # Split the string into a list and take only the last segment of each key\n",
    "    key_list = [k.strip().split('.')[-1] for k in full_keys_str.split(\",\") if k.strip()]\n",
    "    # Optionally, remove duplicates and sort the keys\n",
    "    unique_keys = sorted(set(key_list))\n",
    "    return \", \".join(unique_keys)\n",
    "\n",
    "def get_referenced_json_keys_without_parents(data_value):\n",
    "    full_keys_str = get_referenced_json_keys(data_value)\n",
    "    key_list = [k.strip().split('.')[-1] for k in full_keys_str.split(\",\") if k.strip()]\n",
    "    unique_keys = sorted(set(key_list))\n",
    "    return \", \".join(unique_keys)\n",
    "\n",
    "def get_schema_key_count(schema_value):\n",
    "    # Get the comma-separated string of keys without parent paths.\n",
    "    keys_str = get_sorted_schema_keys_without_parents(schema_value)\n",
    "    # Split into a list and strip any extra whitespace, then create a set to deduplicate.\n",
    "    keys_set = set(k.strip() for k in keys_str.split(\",\") if k.strip())\n",
    "    return len(keys_set)\n",
    "\n",
    "def get_referenced_json_key_count(data_value):\n",
    "    # Get the comma-separated string of keys without parent paths.\n",
    "    keys_str = get_referenced_json_keys_without_parents(data_value)\n",
    "    # Split into a list, strip whitespace, and deduplicate.\n",
    "    keys_set = set(k.strip() for k in keys_str.split(\",\") if k.strip())\n",
    "    return len(keys_set)\n",
    "\n",
    "def create_key_comparison_aligned(row):\n",
    "    # Get lists of keys from each column (assuming they're commaâ€‘separated strings)\n",
    "    schema_keys = {k.strip() for k in row[\"schema_keys\"].split(\",\") if k.strip()}\n",
    "    data_keys   = {k.strip() for k in row[\"referenced_json_keys\"].split(\",\") if k.strip()}\n",
    "    # Create a sorted list of all keys from both sets\n",
    "    all_keys = sorted(schema_keys.union(data_keys))\n",
    "    \n",
    "    rows = []\n",
    "    for key in all_keys:\n",
    "        rows.append({\n",
    "            \"Key\": key,\n",
    "            \"Schema Key\": key if key in schema_keys else \"\",\n",
    "            \"Referenced JSON Key\": key if key in data_keys else \"\"\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_0_100.csv, shape: (100, 13)\n",
      "Loaded /Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_100_200.csv, shape: (100, 13)\n",
      "Error loading /Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_200_300.csv: [Errno 2] No such file or directory: '/Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_200_300.csv'\n",
      "Error loading /Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_300_400.csv: [Errno 2] No such file or directory: '/Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/01_output_300_400.csv'\n",
      "Combined dataframe shape: (200, 13)\n",
      "Error processing row 56: Invalid \\escape: line 149 column 252 (char 6623)\n",
      "Error processing row 67: Expecting value: line 9 column 23 (char 325)\n",
      "Error processing row 113: Expecting value: line 1 column 1 (char 0)\n",
      "Error processing row 124: Invalid \\escape: line 281 column 293 (char 15125)\n",
      "Error processing row 146: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "flag\n",
       "False    195\n",
       "True       5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load and concat all csv files 01_output_0_100, 01_output_100_200, 01_output_100_200, 01_output_200_300, 01_output_300_400 with a for loop\n",
    "\n",
    "all_dfs = []\n",
    "base_path = '/Users/dan.rambado/Documents/reviewing_json_schema_dataset/output/existing_981_tasks/'\n",
    "file_ranges = [(0, 100), (100, 200), (200, 300), (300, 400)]\n",
    "\n",
    "for start, end in file_ranges:\n",
    "    file_path = f\"{base_path}01_output_{start}_{end}.csv\"\n",
    "    try:\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "        all_dfs.append(temp_df)\n",
    "        print(f\"Loaded {file_path}, shape: {temp_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "if all_dfs:\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Combined dataframe shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate\")\n",
    "\n",
    "\n",
    "def safe_analyze(row):\n",
    "    json_str = row['REFERENCED_JSON_FORMATED']\n",
    "    # Debug: Print problematic rows\n",
    "    if not json_str or not json_str.strip():\n",
    "        print(f\"Row {row.name} has an empty or whitespace JSON string.\")\n",
    "        return {}\n",
    "    try:\n",
    "        return analyze_json_and_schema(json_str, row['schema'])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error processing row {row.name}: {e}\")\n",
    "        return {}  # Return a default value when JSON decoding fails\n",
    "\n",
    "df['summary'] = df.apply(lambda row: json.dumps(safe_analyze(row), indent=2), axis=1)\n",
    "\n",
    "\n",
    "#add a column flag = true if  \"prompt_related_score\": 0,\" in summary column\n",
    "df['flag'] = df['summary'].str.contains('\"prompt_related_score\": 0,')\n",
    "\n",
    "df['flag'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['languageCode', 'INTERNAL_ID', 'CUSTOMER_ID', 'PARSED_CUST_ID',\n",
       "       'TASK_ID', 'BATCH_ID', 'prompt', 'schema', 'response',\n",
       "       'MR_EVAL_SUB_SCHEMA', 'REFERENCED_JSON', 'SCHEMA_SIMPLIFIED',\n",
       "       'REFERENCED_JSON_FORMATED', 'summary', 'flag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'REFERENCED_JSON_FORMATED': 'reference_JSON',\n",
    "                   'INTERNAL_ID':'internal_id',\n",
    "                   'TASK_ID': 'TASK'}, inplace=True)\n",
    "\n",
    "# keep the columns needed\n",
    "df = df[['TASK','languageCode', 'internal_id', 'prompt', 'schema', 'reference_JSON', 'summary']]\n",
    "\n",
    "\n",
    "# remove rows that contains \"prompt_related_score\": 0,\" in summary column\n",
    "df = df[~df['summary'].str.contains('\"prompt_related_score\": 0,')]\n",
    "\n",
    "# replace all the value in 'languageCode' for \"en_US\"\n",
    "df['languageCode'] = 'en_US'\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(f'{base_path}v2_batch_005_mar5.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
